"""Generate the dynamic cluster configuration for tf.estimator"""

import json
import os

import tensorflow as tf
from tensorflow.core.protobuf import cluster_pb2
from tensorflow.python.estimator import training as estimator_training
from tensorflow.python.platform import tf_logging as logging
from tensorflow.python.training import server_lib


# pylint: disable=protected-access
def prepare_dynamic_run_config(**kwargs):
    """Prepare a RunConfig for tf.estimator of dynamic cluster."""
    env_tf_config = os.getenv("TF_CONFIG", None)
    if not env_tf_config:
        raise ValueError("Invalid TF config")
    tf_config = json.loads(env_tf_config)
    task_type = tf_config["task"]["type"]
    task_index = tf_config["task"]["index"]

    if task_type == "evaluator":
        config = tf.estimator.RunConfig(**kwargs)
        return config
    del os.environ["TF_CONFIG"]

    master_addr = tf_config["cluster"][task_type][task_index]
    config = tf.estimator.RunConfig(**kwargs)
    config._task_id = task_index
    config._task_type = task_type
    config._master = "grpc://" + master_addr
    config._server_name = "grpc://" + master_addr
    config._protocol = "grpc"
    config._is_chief = config._task_type == "chief"
    cluster_spec = get_cluster_spec(tf_config)
    cluster_spec = filter_cluster_spec(task_type, task_index, cluster_spec)
    config._num_ps_replicas = len(cluster_spec.get("ps", {}))
    config._num_worker_replicas = len(cluster_spec.get(task_type, {}))
    cluster_def = create_cluster_def(cluster_spec)
    config._session_config = get_session_config(cluster_def)
    config._cluster_spec = server_lib.ClusterSpec(cluster_spec)
    init_local_server(config.master)
    return config


def init_local_server(host):
    """Start a local serve"""
    server = server_lib.Server(
        {"local": [host]}, protocol="grpc", config=None, start=None
    )
    logging.info("Start Tensorflow server.")
    server.start()
    server_lib.local_server = server
    estimator_training._TrainingExecutor._start_std_server = _start_std_server


def get_session_config(cluster_def):
    """Get session config using cluster_def.
    Args:
      cluster_def: cluster_pb2.ClusterDef(), which is generated by cluster_spc
        like {"ps": {0: "localhost:1"}, "chief": {0: "localhost:2"}}
    """
    session_config = tf.ConfigProto(
        cluster_def=cluster_def,
        gpu_options=tf.GPUOptions(allow_growth=True),
        allow_soft_placement=True,
        log_device_placement=False,
    )
    exp = session_config.experimental
    exp.share_session_state_in_clusterspec_propagation = True
    return session_config


def get_cluster_spec(tf_config):
    """Get cluster spec from TF_CONFIG
    Args:
    tf_config: The tf_config is in TF_CONFIG environment variable, like
      {
        cluster: {
          "ps":["localhost:0"],
          "chief":["localhost:1"],
          "worker":["localhost:2", "localhost:3"],
        }
        task: {"type": "worker", "task_index": 0}
      }
    Returns: A dict like
      {
          "ps":{0: "localhost:0"},
          "chief":{0: "localhost:1"},
          "worker":{0: "localhost:2", 1: "localhost:3"},
        }
    returns
    """
    cluster_spec = {}
    for task_type in tf_config["cluster"]:
        cluster_spec[task_type] = {}
        for i, host in enumerate(tf_config["cluster"][task_type]):
            cluster_spec[task_type][i] = host
    return cluster_spec


def filter_cluster_spec(task_type, task_index, cluster_spec):
    """keep all ps servers and worker-self.
    Args:
      task_type: "ps", "chief", "worker".
      task_index: int.
      cluster_spec: a dict like
        {
          "ps":{0: "localhost:0"},
          "chief":{0: "localhost:1"},
          "worker":{0: "localhost:2", 1: "localhost:3"},
        }
    returns:
      task_type = "worker" and task_index = 0:
        {
          "ps":{0: "localhost:0"},
          "worker":{0: "localhost:2"},
        }
      task_type = "chief" and task_index = 0:
        {
          "ps":{0: "localhost:0"},
          "chief":{0: "localhost:1"},
        }
      task_type = "ps" and task_index = 0:
        {
          "ps":{0: "localhost:0"},
        }
    """

    import copy

    new = copy.deepcopy(cluster_spec)

    if task_type == "worker":
        del new["chief"]
        for i in list(new.get("worker", {})):
            if i != task_index:
                del new["worker"][i]
    elif task_type == "chief":
        del new["worker"]
    elif task_type == "ps":
        del new["worker"]
        del new["chief"]
        for i in list(new.get("ps", {}).keys()):
            if i != task_index:
                del new["ps"][i]
    return new


def create_cluster_def(cluster_spec):
    cluster_def = cluster_pb2.ClusterDef()
    for job_name in cluster_spec:
        job = cluster_def.job.add()
        job.name = job_name
        for i in cluster_spec[job_name]:
            job.tasks[i] = cluster_spec[job_name][i]
    return cluster_def


# pylint: disable=unused-argument
def _start_std_server(self, config):
    """Creates, starts, and returns a server_lib.Server."""

    if (
        not config.cluster_spec
        or not config.task_type
        or config.task_id is None
    ):
        raise RuntimeError(
            "Could not start server; be sure to specify "
            "cluster_spec, task_type, and task in "
            "RunConfig or set the TF_CONFIG environment variable."
        )

    if not config.master:
        jobs = config.cluster_spec.jobs
        if (
            len(jobs) == 1
            and len(config.cluster_spec.job_tasks(jobs[0])) == 1
            and config.task_type in estimator_training._TRAINER_JOBS
        ):
            # For distributed training, config.master is empty if and only if it has
            # a single node in the cluster spec. In this case, we should not start
            # the server.
            # logger.info(
            #   "Skip starting Tensorflow server as there is only one "
            #   "node in the cluster."
            # )
            return None
        raise RuntimeError(
            "Could not start server; be sure to specify master in "
            "RunConfig or set the TF_CONFIG environment variable."
        )

    return server_lib.local_server
